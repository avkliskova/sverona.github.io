<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>2019 on S. Verona Malone</title><link>./tags/2019/</link><description>Recent content in 2019 on S. Verona Malone</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Tue, 07 May 2019 00:00:00 +0000</lastBuildDate><atom:link href="./tags/2019/index.xml" rel="self" type="application/rss+xml"/><item><title>Neural networks in time scale calculus</title><link>./blog/nabla/</link><pubDate>Tue, 07 May 2019 00:00:00 +0000</pubDate><guid>./blog/nabla/</guid><description>Neural networks are a popular technique for machine learning problems such as automated handwriting recognition, automated speech recognition, and time-series forecasting. On these tasks, so-called deep neural networks [Schmidhuber 2015] and convolutional neural networks [Ciresan et al.Â 2011] have performed at least as well as the best known non-neural-network statistical learning techniques. A typical neural network numerically solves an extremization problem using gradient descent in a high-dimensional Euclidean space using an algorithm known as backpropagation.</description></item><item><title>The dual simplex method, an introduction</title><link>./blog/dual-simplex/</link><pubDate>Mon, 01 Apr 2019 00:00:00 +0000</pubDate><guid>./blog/dual-simplex/</guid><description>The primal simplex method provides an algorithmic means for solving linear programs (hereinafter LPs.) This document motivates, defines, and applies the so-called dual simplex method, a slight variant which is useful for performing post hoc sensitivity analysis on LPs solved by the simplex method.
The primal simplex method In this section, we review the three steps of the usual primal method, following [Hillier-Lieberman], Frederick S. Hillier and Gerald J. Lieberman, Introduction to operations research, 10th ed.</description></item></channel></rss>